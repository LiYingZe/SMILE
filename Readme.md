# ğŸ˜„ SMILE: Small Language Model Integrated LIKE Engine

---

## ğŸš€ Why accelerate the **LIKE** predicate with language models?

Modern databases often use the `LIKE` predicate to search text data. However, when the search condition is broken up by wildcards (`%`, `_`), existing search structures can degrade to the worst-case linear scan ğŸŒ, leading to poor performance. Traditional methods like B+-trees ğŸŒ³ struggle when wildcards appear on both ends of the pattern.

Recent advances in the language-model world ğŸ’¡ open up a very promising new path. These models can â€œunderstandâ€ and **decode** complex `LIKE` patterns, converting them into a small set of candidate values ğŸ¤–ğŸ”, which can then be verified via hash-table lookups in constant time âš¡â€”dramatically improving efficiency! Butâ€¦ integrating an LLM directly into a database is **difficult** due to high latency â±ï¸, large storage overhead ğŸ—„ï¸, and sensitivity to data drift ğŸ¯.

---

## ğŸ¤– Meet SMILE: Small Language Model Integrated LIKE Engine ğŸ˜„

![SMILE](./SMILE.png)

SMILE learns **column-local character distributions** using small yet refined parameters âœ¨. It acts as a lightweight â€œneural translatorâ€ ğŸ”„, translating `LIKE` patterns into a candidate setâ€”fast ğŸï¸, accurate ğŸ¯, and lightweight ğŸª¶.

### ğŸ† Why SMILE?

We evaluate SMILE on multiple datasets (e.g., TPC-H, IMDB, Reddit) and compare it against
PostgreSQL native indexes (B-tree, GIN, GiST) as well as large language models (e.g., DeepSeek-V3, Qwen2.5).

* ğŸš€ **Stunning speed**: up to **1000Ã—** faster than sequential scan (SeqScan), and **1.8â€“41.6Ã—** faster than
  PostgreSQL **GIN (Trigram)** index.
* ğŸ¯ **High recall**: under complex query patterns, LLM-based approaches often achieve **<10%** recall,
  while **SMILE consistently reaches 90%â€“95%+**.
* ğŸ—„ï¸ **Huge storage savings**: compared to **GIN** and **GiST** indexes, SMILE reduces space usage by **23â€“82Ã—**.


---

## ğŸ—‚ï¸ Code Structure

```text
.
â”œâ”€â”€ data                           ğŸ“ Demo datasets
â”‚   â”œâ”€â”€ lineitem.csv               ğŸ§¾ Sampled TPCH-lineitem data
â”‚   â””â”€â”€ wiki.csv                   ğŸ“š Wiki text data
â”œâ”€â”€ E2E_Exp                        âš™ï¸  End-to-end experiment scripts (PostgreSQL)
â”‚   â”œâ”€â”€ create_index.py            ğŸ› ï¸  Create index / export data
â”‚   â”œâ”€â”€ generateworkload.py        ğŸ²  Generate LIKE query workloads
â”‚   â””â”€â”€ run_workload.py            ğŸš€  Run workloads and evaluate
â”œâ”€â”€ models                         ğŸ¤– Pretrained model parameters
â”‚   â”œâ”€â”€ lineitem                   ğŸ“¦ lineitem model
â”‚   â”‚   â””â”€â”€ best_model.pth         âœ… Pretrained weights
â”‚   â””â”€â”€ wiki                       ğŸ“¦ wiki model
â”‚       â””â”€â”€ best_model.pth         âœ… Pretrained weights
â”œâ”€â”€ SLM_LIKE.py                    ğŸ‹ï¸  Model training entry
â”œâ”€â”€ evaluate.py                    ğŸ Model evaluation entry
â”œâ”€â”€ chat_inference.py              ğŸ’¬ Interactive inference entry
â”œâ”€â”€ requirements.txt               ğŸ“¦ Python dependencies
â””â”€â”€ Realworld_84047LIKE.csv        ğŸŒ 84,047 real-world LIKE scenarios
```

---

## ğŸ“¦ Dependency Requirements

See `requirements.txt` for the full dependency list.

---

## ğŸ› ï¸ Environment Setup

### 1) Create an environment

```bash
conda create -n SMILE python=3.9 -y
conda activate SMILE
```

### 2) Install dependencies

```bash
python -m pip install --upgrade pip
pip install -r requirements.txt
```

---

## ğŸ“ Datasets

Data available directly in the repository:

* `./data/lineitem.csv`
* `./data/wiki.csv`

We also added **84,047 real-world LIKE predicate usage scenarios from 30 datasets** in `Realworld_84047LIKE.csv`.

### Open datasets used in the paper

* **IMDB** (primaryName, ~15M): [https://datasets.imdbws.com/](https://datasets.imdbws.com/)
* **WIKI** (titles, ~4M): [https://dumps.wikimedia.org/enwiki/](https://dumps.wikimedia.org/enwiki/)
* **TPC-H** (lineitem.comment, ~24M): [https://www.tpc.org/tpch/](https://www.tpc.org/tpch/)
* **Reddit** (usernames, ~2M): [https://www.kaggle.com/datasets/colinmorris/reddit-usernames](https://www.kaggle.com/datasets/colinmorris/reddit-usernames)
* **RedPajama** (text windows, ~1B): [https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt](https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt)
* **Newsroom** (news articles, ~1.21M): [https://lil.nlp.cornell.edu/newsroom/download/index.html](https://lil.nlp.cornell.edu/newsroom/download/index.html)

---

## ğŸ“ˆ Evaluate SMILE

### Evaluate with the pretrained lineitem model

```bash
python evaluate.py --model_path ./models/lineitem/best_model.pth
```

### Evaluate with the pretrained wiki model

```bash
python evaluate.py --data_path ./data/wiki.csv --model_path ./models/wiki/best_model.pth
```

---

## ğŸ§ª Train SMILE

You can train SMILE yourself on an affordable 8GB GPU and easily verify its performance. Itâ€™s very convenient and cost-effectiveâ€”you can get started right away. Once you plug in the model, youâ€™ll see the magic immediately! ğŸŒŸ

```bash
python SLM_LIKE.py
```

ğŸ’¡ *Tip: Adjust `--inPct` and `--pct` in the code to control the proportion of queries included and the wildcard ratio.*

After training, the model will be saved to:

* `./models/<saveName>/...` (depending on the scriptâ€™s actual saving logic)

---

## ğŸ’¬ Interactive LIKE Pattern Prediction (Interactive Inference) ğŸ¤–âœ¨

We provide an **interactive program** ğŸ•¹ï¸ that lets you enter an SQL `LIKE` pattern ğŸ” (e.g., `%dam La_berth%`, `%mit Sur_avanshi%`, `%ichael _empson%`, `%lexander _ohnson%`, `%aia R_ssell%`), and our SMILE model ğŸ˜„âš¡ will produce **instant predictions** as matching results.

Just type your pattern and press Enter âŒ¨ï¸â€”our lightweight neural engine will return predicted matches in real time ğŸ¯!

You can exit anytime by entering `'exit'` or `'q'` âŒğŸ‘‹.

### â–¶ï¸ How to run (lineitem)

```bash
python chat_inference.py --data_path ./data/lineitem.csv --model_path ./models/lineitem/best_model.pth --inferSampleNum 4
```

### âœ… Inference example (real runtime output)

Below is an example of a real interactive inference session: you input a complex pattern containing `%` and `_`, and the model returns the Top-4 candidate results and latency.

```text
Model loaded. Enter a LIKE pattern (e.g., a%cd_). Type 'exit' or 'q' to quit.

LIKE pattern > %aia R_ssell%


Results (Top 4):
1. Maia Rossell
2. Maia Rossell
3. Maia Rossell
4. Maia Rossell
Time: 0.16 s

LIKE pattern > exit
Exiting.
```

### â–¶ï¸ How to run (wiki, optional)

```bash
python chat_inference.py --data_path ./data/wiki.csv --model_path ./models/wiki/best_model.pth --inferSampleNum 4
```

---

### ğŸ§™ What it does:

* ğŸ—£ï¸ **Chat with SMILE**: easily input `LIKE` patterns in a natural way.
* âš¡ **Instant results**: get predictions in the blink of an eye.
* ğŸ¯ **Real-world queries**: test on real entries from the `lineitem` dataset.
* ğŸ’¡ **Smart matching**: handles wildcards `%` and `_` in an intelligent learned way.
* ğŸ“Š **Neural LIKE acceleration**: simulates queries in real scenarios (e.g., search engines) such as
  `SELECT * FROM lineitem WHERE comment LIKE "%keyword%" LIMIT K`.

---

> ğŸ¤– Tip: `%` matches any number of characters, and `_` matches exactly one character.

> ğŸ§© *Behind the scenes*: your pattern is sent to our small-but-powerful SMILE model ğŸ¤–, which predicts the set of rows that match your pattern like a translatorâ€”**much faster** than scanning the entire column linearly. ğŸ”¥

---

## ğŸ§± End-to-end scripts (E2E_Exp)

`E2E_Exp/` includes:

* `create_index.py`: requires connecting to PostgreSQL, and uses `--csv_filename` to specify the output file
* `generateworkload.py`: query generator
* `run_workload.py`: executes evaluation

Notes:

* The input/output formats and default paths of these scripts may need to be adjusted to fit your experimental environment before running.

Whether youâ€™re debugging, testing queries, or just curiousâ€”this workflow lets you explore SMILE in a fun and interactive way! ğŸ¤“ğŸ‰

Ready to chat with your database? ğŸ’¬ğŸ“Š Let the LIKE magic begin! âœ¨ğŸª„

Ready to make your database **smile**? ğŸ˜„
Let the neural LIKE acceleration begin! âš¡ğŸ¤–ğŸ“š