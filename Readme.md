------

# ğŸš€ **The Case For Language Model Accelerated** **LIKE** **Predicate**

Modern databases often use the `LIKE` predicate to search text data. However, when the search condition is interrupted by wildcards (`%`, `_`), the existing search structure can degrade into a worst-case linear scan ğŸŒ, resulting in poor performance. Traditional methods like B+-trees ğŸŒ³ struggle when wildcards appear at **both ends** of the pattern.

Recent advances in language models ğŸ’¡ open a promising new path. These models can "understand" and **decode** complex `LIKE` patterns into a small set of candidate values ğŸ§ ğŸ”, which are then verified in constant time using hash table lookups âš¡â€”greatly improving efficiency! Butâ€¦ integrating LLMs directly into databases is **hard** due to high latency â±ï¸, large storage ğŸ—„ï¸, and sensitivity to data drift ğŸ¯.

------

### ğŸ§  Meet **SMILE**: *Small language Model Integrated LIKE Engine* ğŸ˜„

SMILE learns **column-local character distributions** through small but exquisite parameters âœ¨. It acts as a *neural translator* ğŸ”„ that converts `LIKE` patterns into their corresponding result setsâ€”fast ğŸï¸, precise ğŸ¯, and lightweight ğŸª¶.

### ğŸ† Why SMILE?

- ğŸš€ Up to **1000x faster** than sequential scans & large LLMs
- ğŸ”¥ **44â€“141x faster** than trigram indexes
- âš¡ **100x faster** than B+-trees
- ğŸ¤– Only a **tiny model** (5 orders of magnitude smaller than GPT-scale models!)
- ğŸ’ª Robust against data/query distribution drifts

------

## ğŸ—‚ï¸ Code Structure

Our project is organized as follows:

```
.
â”œâ”€â”€ data                           ğŸ“ Demo dataset (Sampled TPCH-lineitem)
â”œâ”€â”€ E2E_Exp                        âš™ï¸  End-to-end experiment scripts
â”‚   â”œâ”€â”€ create_index.py            ğŸ› ï¸  Index creation
â”‚   â”œâ”€â”€ generateworkload.py        ğŸ²  Query generator
â”‚   â””â”€â”€ run_workload.py            ğŸš€  Execute evaluation
â”œâ”€â”€ evaluate.py                    ğŸ Model evaluation
â”œâ”€â”€ SLM_Like.py                    ğŸ‹ï¸ Model training
â””â”€â”€ models                         ğŸ§ Trained model parameters
```

------

## ğŸ“¦ Requirements

- ğŸ Python 3.9.13
- ğŸ”¥ PyTorch 2.5.1
- ğŸ”¢ Numpy 2.0.2
- ğŸ“Š Pandas 1.5.1

------

## ğŸ“¥ Setup & Dataset

For the convenience of demonstration, we have provided a sample of 10,000 records from the TPC-H lineitem table. ğŸ˜Š 
We denote it as `lineitem10000` ğŸ“„, which contains  10,000 entries from the `l_comment` column of the TPC-H `lineitem` table. Saved as `lineitem10000.csv` in the `data` directory.More datasets can be referred to through our links. ğŸ“š

## ğŸ“§ Emails

For the **Emails** dataset, we use `gen_email.py` to generate the data ğŸ› ï¸. After generation, **deduplication** is required to ensure clean and unique entries ğŸ§¹âœ¨.

â–¶ï¸ Run the following commands to generate and clean the data:

```
python gen_email.py     # ğŸ“§ Generate email samples
python duplicate.py     # ğŸ§¼ Deduplicate entries
```

âœ… Now your email dataset is fresh, clean, and ready to go! ğŸš€

------

## ğŸ“± Phone Numbers

For the **Phone_numbers** dataset, we use `gen_phone_data.py` to generate the data ğŸ“². Similar to the emails, **deduplication** is necessary after generation to avoid noisy duplicates ğŸ§½.

â–¶ï¸ Use the following commands:

```
python gen_phone_data.py  # ğŸ“± Generate phone number samples
python duplicate.py       # ğŸ§¼ Deduplicate entries
```

ğŸ“¦ Once cleaned, the phone number dataset is good to go for training, testing, or querying! ğŸ’ªğŸ“Š

------

## ğŸ§ª Train SMILE

You can use our pre-trained model (W1 setting) âœ… and train your own SMILE with an 8GB budget-friendly GPU to easily verify it. It's super convenient and cost-effective, and you can get started right away. Just plug in the model and see the magic happen! ğŸŒŸ

```bash
python SLM_Like.py   --lr 0.0003   --batch_size 1024   --inPct 0.1   --pct 0.2   --saveName lineitem10000_lr0.0003_in1Pct2   --data_path ./data/lineitem10000.csv  --GPU 0 
```

ğŸ’¡ *Tip: Adjust `--inPct` and `--pct` to control query inclusion and wildcard percentage.*

------

## ğŸ“ˆ Evaluate SMILE

Evaluate the trained model with:

```bash
python evaluate.py   --PathOfModel ./models/lineitem10000_lr0.0003_in1Pct2/Ep_999_Seq2Seq  --inPct 0.1   --pct 0.2
```

ğŸ” This runs LIKE queries  for different LIKE Workloads ğŸ“Š.

------

## ğŸ’¬ Interactive LIKE Pattern Prediction ğŸ§ âœ¨

We provide an **interactive program** ğŸ•¹ï¸ that allows you to input a SQL `LIKE` pattern ğŸ” (e.g., `%fox%`, `__quick`, `lazy%`) and get **instant predictions** powered by our SMILE model ğŸ˜„âš¡.

Just type your pattern and hit enter âŒ¨ï¸â€”our lightweight neural engine will return the predicted matching results ğŸ¯ in real-time!

You can **exit anytime** by typing `'exit'` or `'q'` âŒğŸ‘‹.

------

### ğŸ§ª Try it Yourself!

Launch the program using this command:

```
python chat_inference.py --data_path "./data/lineitem10000.csv" --PathOfModel "./models/lineitem10000_lr0.0003_in1Pct2/Ep_9999_Seq2Seq"
```

------

### ğŸ§™ What it does:

- ğŸ—£ï¸ **Talk to SMILE**: Enter `LIKE` patterns naturally
- âš¡ **Fast Results**: Get predictions almost instantly
- ğŸ¯ **Real Queries**: Test against actual `lineitem` dataset entries
- ğŸ’¡ **Smart Matching**: Handles wildcards `%` and `_` with learned intelligence

------

> ğŸ§© *Behind the scenes*: Your pattern is passed through our small but mighty SMILE model ğŸ¤–, which acts as a translator to predict the set of rows that match your patternâ€”**way faster** than scanning the whole column linearly. ğŸ”¥

------

Whether you're debugging, testing queries, or just curiousâ€”this mode makes SMILE fun and interactive to explore! ğŸ¤“ğŸ‰

Ready to chat with your database? ğŸ’¬ğŸ“Š Let the LIKE magic begin! âœ¨ğŸª„

Ready to make your databases **smile**? ğŸ˜„
 Let neural LIKE acceleration begin! âš¡ğŸ§ ğŸ“š

